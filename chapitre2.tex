\section{Chapitre 2: Matrices}

\subsection{Définition d'une matrice}
\noindent
\begin{definition}
    L'ensemble des matrices de type $m \times n$ sur un corps $F$ est noté $M_{m \times n}(F)$ \\
    Si $F$ est un corps quelconque alors $M_{m \times n}(F) = M_{m \times n}$
\end{definition}
\begin{definition}
    Soit $A \in M_{m \times n}(F)$. $A$ est aussi noté $A_{m \times n}$ et peut-être représenté
    par un tableau qui contient $m$ lignes et $n$ colonnes ou $\left(A\right)_{kj}$ est l'élément
    à la ligne $k$ et colonne $j$. \[
        A = A_{m \times n} = \begin{pmatrix}
            a_{11} & a_{12}      & \dots  & a_{1n} \\
            a_{21} & a_{22}      & \dots  & a_{2n} \\
            \vdots & \phantom{a} & \ddots & \vdots \\
            a_{m1} & a_{m2}      & \dots  & a_{mn}
        \end{pmatrix}, \; \left(A\right)_{kj} \in F \ \forall \ k, j
    \]
    Notez que dans ce cours, on considère seulement le cas $F = \R$ ou $F = \C$
\end{definition}
\begin{definition}
    Si $m = n$ alors $A_{m \times n} = A_{nn} = A_n$ est une matrice carré.
\end{definition}
\begin{definition}
    La matrice composé uniquement de 0 pour un certain type $m \times n$ est écrite $\mathbb{O}$
\end{definition}
\begin{definition}
    La matrice identité de type $n$ est écrite $I_n = I$ avec $(I)_{kj} = \begin{cases}
            1 & \text{si} \ i = j    \\
            0 & \text{si} \ i \neq j
        \end{cases}$
\end{definition}
\noindent
Soit $A \in M_n(F)$ et $a_{kj} \in F$ avec $k, j \in \{0, 1, \dots, n\}$
\begin{definition}
    $A$ est triangulaire supérieur si $(A)_{kj} = \begin{cases}
            a_{kj} & \text{si} \ j \geq k \\
            0      & \text{si} \ j < k
        \end{cases}$
\end{definition}
\begin{definition}
    $A$ est triangulaire inférieur si $(A)_{kj} = \begin{cases}
            a_{kj} & \text{si} \ j \leq k \\
            0      & \text{si} \ j > k
        \end{cases}$
\end{definition}
\begin{remark}
    $A$ est triangulaire si $A$ est triangulaire supérieur ou triangulaire inférieur
\end{remark}
\begin{definition}
    $A$ est diagonale si $(A)_{kj} = \begin{cases}
            a_{kj} & \text{si} \ i = j    \\
            0      & \text{si} \ i \neq j
        \end{cases}$
    \begin{remark}
        $I_n = \text{diag}\{ 1, 1, \dots, 1 \}$ ainsi que $\mathbb{O}_n = \text{diag}\{ 0, 0, \dots, 0 \}$
    \end{remark}
\end{definition}


\subsection{Opérations matricielles}

\subsubsection{Addition de matrices}
\noindent
L'addition de deux matrices $A$ et $B$ existe seulement si $A$ et $B$ sont du même type.
L'addition est commutatif, associatif, possède un élément neutre et possède un inverse.
\[ (AB)_{kj} = a_{kj} + b_{kj} \]

\subsubsection{Multiplication par un scalaire}
\noindent
La multiplication par un scalaire $\alpha$ existe pour une matrice $A$ de n'importe quel type. La multiplication
par un scalaire est associatif, distributif, commutatif et possède un inverse si $\alpha \neq 0$.
\[ (\alpha A)_{kj} = \alpha a_{kj}\]

\subsubsection{Produit matricielle}
\noindent
Soit $A \in M_{m \times n}$ et $B \in M_{r \times t}$. Alors
le produit matricielle $AB$ existe si $n = r$. Dans ce cas, on a \[
    (AB)_{kj} = \sum_{s = 1}^{n} a_{ks} b_{sj}
\]
\begin{remark}
    L'élément à la position $i, j$ de $AB$ est le produit scalaire canonique entre la i-ème ligne
    de A et la j-ème colonne de B
\end{remark}

\subsubsection{Propriétés du produit matricielle}
\noindent
Soit $A, B, C$ des matrices dont le produit matricielle entre eux existe. \\
Le produit matricielle est associatif, distributif et possède un élément neutre, soit $I$. \\
\underline{Important}: Le produit matricielle n'est, généralement, \underline{pas commutatif}
\[
    \begin{matrix}
        (AB)C = A(BC) & AB \neq BA & A(B + C) = AB + AC & (A + B)C = AC + BC & AI = IA = A
    \end{matrix}
\]

\subsubsection{Transposition}
\begin{definition}
    La transposé de $A \in M_{m \times n}$ est noté $A^T$ avec $\left(A^T\right)_{kj} = a_{jk}$.
    \begin{remark}
        Pour une matrice carré, la transposé est une rotation des anti-diagonale par rapport à la grande diagonale.
    \end{remark}
\end{definition}

\subsubsection{Conjugée hermitien}
\begin{definition}
    Le conjugué hermitien de $A \in M_{m \times n}$ est noté $A^\dagger$ avec $A^\dagger = \left(A^T\right)^\star = \left(A^\star\right)^T$.
    Le conjugué hermitien est aussi appellé la transposé conjugué.
    \begin{remark}
        Pour $A \in M_{m \times n}(\R)$, $A^\dagger = A^T$
    \end{remark}
\end{definition}

\subsubsection{Propriétés transposé et conjugué hermitien}
\noindent
Soit $A, B \in M_n$, $\scalaire{\alpha}$. Alors on a
\[
    \begin{matrix}
        \left( A^T \right)^T = A               & \left( A^\dagger \right)^\dagger = A                     \\[0.5em]
        \left( \alpha A \right)^T = \alpha A^T & \left( \alpha A \right)^\dagger = \alpha^\star A^\dagger \\[0.5em]
        \left( A + B \right)^T = A^T + B^T     & \left( A + B \right)^\dagger = A^\dagger + B^\dagger     \\[0.5em]
        \left( AB \right)^T = B^TA^T           & \left( AB \right)^\dagger = B^\dagger A^\dagger
    \end{matrix}
\]
\begin{definition}
    Si $A^T = A$, alors $A$ est dite symétrique. Si $A^T = -A$, alors $A$ est dite anti-symétrique.
\end{definition}
\begin{definition}
    Si $A$ est une matrice carré tel que $AA^T = A^TA = I$, alors $A$ est appellé orthogonal.
\end{definition}
\begin{definition}
    Si $A^\dagger = A$, alors $A$ est dite hermitienne. Si $A^\dagger = -A$, alors $A$ est dite anti-hermitienne.
\end{definition}
\begin{definition}
    Si $A$ est une matrice carré tel que $AA^\dagger = A^\dagger A = I$, alors $A$ est appellé unitaire.
\end{definition}
\begin{remark}
    La symétrie et l'orthogonal s'applique au matrice réel.
    L'hermitien et l'unitaire s'applique au matrice complexe.
\end{remark}

\subsubsection{Commutateur de matrice}
\noindent
Soit $A, B \in M_n(\mathbb{C})$, alors le commutateur de $A$ et $B$ est la matrice
\[ \left[ A, B \right] = AB - BA \]

\subsubsection{Propriétés du commutateur}
\begin{enumerate}
    \item $[A, B] = -[B, A]$
    \item $[\alpha A, B ] = [A, \alpha B] = \alpha [A, B]$
    \item $[A + B, C] = [A, C] + [B, C]$
    \item $[A, B]^T = -[A^T, B^T]$
    \item L'identité de Jacobi: $[[A, B], C] + [[C, A], B] + [[B, C], A] = \mathbb{O}$
\end{enumerate}

\subsubsection{Trace d'une matrice}
\noindent
Soit $A = (a_{ij})_{n \times n}$. Alors la trace de $A$ est
\[ \text{tr} A =  \sum_{j = 0}^{n} a_{jj} \]

\subsubsection{Propriétés de la trace}
\begin{enumerate}
    \item $\text{tr}\{ AB \} = \text{tr}\{ BA \}$
    \item $\text{tr}\{ A + B \} = \text{tr}A + \text{tr}B$
    \item $\text{tr}\{ \alpha A \} = \alpha \text{tr}A $
    \item $\text{tr}\{ A^T \} = \text{tr}A$, $\text{tr}\{ A^\dagger \} = \text{tr}A^\star$
    \item $\text{tr}\{ [A, B] \} = \text{tr}\{ AB - BA \} = \text{tr}\{ AB \} - \text{tr}\{BA\} = \text{tr}\{ AB \} - \text{tr}\{AB\} = 0$
\end{enumerate}

\subsection{Déterminant d'une matrice carré}

\subsubsection{Défintion du déterminant par récurrence}
\begin{definition}
    Le déterminant de $A \in M_n(\C)$ noté $\det A = |A|$ est une nombre complexe
    qui peut-être définie par récurrence sur $n$ comme suit \[
        \det A = \begin{cases}
            a_{11},                                   & \text{si } n = 1 \\
            \sum_{j = 1}^{n} (-1)^{k + j}a_{kj}m_{kj} & \text{si } n > 1
        \end{cases}
    \]
\end{definition}
\noindent
Ceci est l'expansion par la kème ligne, $1 \leq k \leq n$. Le terme $m_{kj}$
est le déterminant de la sous-matrice $M_{kj}$ qu'on obtient si on biffe la ligne $k$ et la
colonne $j$, alors $m_{ji} = \det M_{ji}$. \\
Dans la première équation, le déterminant des sous-matrices était multiplié par la kème ligne. Il est
équivalent de développer le déterminant suivant la jème colonne, ce qui donne cette équation \[
    \det A = \begin{cases}
        a_{11},                                   & \text{si } n = 1 \\
        \sum_{k = 1}^{n} (-1)^{k + j}a_{kj}m_{kj} & \text{si } n > 1
    \end{cases}
\]
\begin{remark}
    La seule différence entre les deux équations est l'indice de la somme. L'indice est $j$ si on
    développe par la kème ligne sinon l'indice est $k$ si on développe par la jème colonne.
\end{remark}

\subsubsection{Défintion du déterminant par les permutations}
\begin{definition}
    Le déterminant de $A$ peut aussi être définie de cette façons \begin{equation*}
        \det A = \sum_{\sigma \in S_n} \text{sign}(\sigma) \prod_{i = 1}^{n}a_{j \sigma_j}
    \end{equation*}
\end{definition}
\noindent
Ici $\sigma = \begin{pmatrix}
        \sigma_1 & \sigma_2 & \dots & \sigma_n
    \end{pmatrix}$ et représente une permutation de l'ensemble $\{1, \ 2, \dots, \ n  \}$. \\
$\sigma_j$ est le jème élément de cette permutation. \\
$S_n$ est l'ensemble de tous ces permutations. \\
$\text{sign}(\sigma)$ est la signature de $\sigma$ et est égale à $(-1)^{\text{nb de désorde de } \sigma}$. \\
Le désorde de $\sigma$ est le nombre de couple $(\sigma_j, \sigma_k)$ ou $j < k$ mais $\sigma_j > \sigma_k$.

\subsubsection{Propriétés du déterminant}
\noindent
Soit $A, B \in M_n(\mathbb{C}), \ \alpha \in \mathbb{C}$ \begin{enumerate}
    \item $\det A^T = \det A$
    \item $\det(\alpha A) = \alpha^n \det(A)$
    \item $\det (A^\star) = (\det A)^\star$
    \item $\det A^\dagger = (\det A)^\star$
    \item $\det(AB) = \det(A) \det(B)$
    \item $\det(A^m) = (\det A)^m$
    \item $\det(A + B) \neq \det A + \det B$, généralement
    \item $\det(\text{diag}\{a_1, \ a_2, \ldots, a_n \}) = a_1 a_2 \dots a_n$
    \item Si $A$ est triangulaire alors $\det A = a_{11} a_{22} \dots a_{nn}$
\end{enumerate}
Il existe aussi des propriétés utile pour transformer le déterminant d'une matrice carré quelconque
en le déterminant d'une matrice triangulaire multiplié par un facteur constant. \\
Soit $A = \arraycolsep=0.3\arraycolsep \begin{bmatrix}
        (L_1)^T & (L_2)^T & \ldots & (L_n)^T
    \end{bmatrix} = \arraycolsep=0.3\arraycolsep \begin{bmatrix}
        C_1 & C_2 & \ldots & C_n
    \end{bmatrix}$ ou $L_j \in M_{1 \times n}(\mathbb{C}), \ C_j \in M_{n\times 1}(\mathbb{C})$ \\
Dans ce cas, $L_j$ est la jème ligne de $A$ et $C_j$ est la jème colonne de $A$.
Les lignes sont écris avec des transposés pour sauver de l'espace. \\
Note: Les propriétés sont valides pour les lignes et les colonnes.
Par contre, pour être concis, ils sont seulement exprimés en termes des colonnes dans ce résumé.
\begin{enumerate}
    \item Échanger deux colonnes ou lignes change le signe du déterminant:
          \[\det \arraycolsep=0.3\arraycolsep \begin{pmatrix}
                  C_1 & \ldots & C_j & \ldots & C_k & \ldots & C_n
              \end{pmatrix} = -\det \begin{pmatrix}
                  C_1 & \ldots & C_k & \ldots & C_j & \ldots & C_n
              \end{pmatrix}\]
    \item Si deux colonnes ou lignes sont pareils alors le déterminant est nulle
          \[\det \arraycolsep=0.3\arraycolsep \begin{pmatrix}
                  C_1 & \ldots & C_j & \ldots & C_j & \ldots & C_n
              \end{pmatrix} = 0\]
    \item Multiplier une colonne ou ligne par un scalaire revient le déterminant par ce même scalaire
          \[\det \arraycolsep=0.3\arraycolsep \begin{pmatrix}
                  C_1 & \ldots & \alpha C_j & \ldots & C_n
              \end{pmatrix} = \alpha \det \begin{pmatrix}
                  C_1 & \ldots & C_j & \ldots & C_n
              \end{pmatrix}\]
    \item Ajouter une colonne ou ligne multiplié par un scalaire à une autre colonne ou ligne
          ne change pas le déterminant
          \[ \det \arraycolsep=0.3\arraycolsep \begin{pmatrix}
                  C_1 & \ldots & C_j & \ldots & C_k & \ldots & C_n
              \end{pmatrix} = \det \begin{pmatrix}
                  C_1 & \ldots & C_j + \alpha C_k & \ldots & C_k & \ldots & C_n
              \end{pmatrix}\]
    \item Si on peut décomposer une colonne ou ligne en la somme de deux colonne ou ligne
          alors le déterminant est la somme des déterminants des deux matrices qui ont chacune
          une décomposition de la colonne ou ligne
          \[ \det \arraycolsep=0.3\arraycolsep \begin{pmatrix}
                  C_1 & \ldots M + N & \ldots & C_n
              \end{pmatrix} = \det \begin{pmatrix}
                  C_1 & \ldots M & \ldots & C_n
              \end{pmatrix} + \det \begin{pmatrix}
                  C_1 & \ldots N & \ldots & C_n
              \end{pmatrix} \]
\end{enumerate}

\subsection{Matrice inverses}
\begin{definition}
    $A \in M_{n \times n}$ est inversible $\iff \exists \ B$ t.q. $AB = BA = I$. \\
    Alors $B$ est noté $A^{-1}$ et est appellé la matrice inverse de $A$
\end{definition}
\subsubsection{Propriétés de l'inverse}
\noindent
Soit $A \in M_{n \times n}(\C)$ inversible \begin{enumerate}
    \item $AB = \mathbb{O} \implies B = \mathbb{O}$
    \item Si $AC = BA = I \implies B = BI = B(AC) = (BA)C = IC = C $, l'inverse à gauche et à droite sont égaux
    \item $(A^{-1})^{-1} = A$, \quad $(A^T)^{-1} = (A^{-1})^{T}$, \quad $(A^\dagger)^{-1} = (A^{-1})^{\dagger}$
    \item $(AB)^{-1} = B^{-1}A^{-1}$
\end{enumerate}

\subsubsection{Calculer la matrice inverse}
\noindent
Soit $A \in M_{n \times n}(\C)$, alors
\[ A^{-1} = \frac{1}{\det A} \ \text{adj}(A) \]
$\text{adj}(A)$ est la matrice adjointe de $A$ et est définie comme
\[ \left(\text{adj}(A)\right)_{kj} = \left( c_{jk} \right) \]
$c_{kj}$ est le cofacteur d'incice $kj$ de $A$ et est défini comme
\[ c_{kj} = (-1)^{k + j}m_{kj} \]
$m_{kj}$ est le déterminant de la sous-matrice obtenue en biffant la ligne $k$ et la colonne $j$. \\
On peut donc remarquer que $\det A = \sum_{j = 1}^{n} = a_{kj} c_{kj}$ \\
Cette manière de calculer la matrice inverse nous permet de formuler ce théorème:
\begin{theorem}
    \label{inversible_small}
    $A$ est inversible $\iff \det A \neq 0$
\end{theorem}

\subsection{Systèmes d'équations linéaires}
\noindent
\begin{definition}
    Chaque système d'équation linéaire peut-être représenter par une equation matricielle, soit
    \[
        \begin{cases}
            a_{11}x_1 + a_{12}x_2 + \ldots + a_{1n}x_n & = b_1 \\
            a_{21}x_1 + a_{22}x_2 + \ldots + a_{2n}x_n & = b_2 \\
            \vdots                                             \\
            a_{m1}x_1 + a_{m2}x_2 + \ldots + a_{mn}x_n & = b_m
        \end{cases} \iff \begin{pmatrix}
            a_{11} & a_{12}      & \dots  & a_{1n} \\
            a_{21} & a_{22}      & \dots  & a_{2n} \\
            \vdots & \phantom{a} & \ddots & \vdots \\
            a_{m1} & a_{m2}      & \dots  & a_{mn}
        \end{pmatrix} \begin{pmatrix}
            x_1    \\
            x_2    \\
            \vdots \\
            x_n
        \end{pmatrix} = \begin{pmatrix}
            b_1    \\
            b_2    \\
            \vdots \\
            b_m
        \end{pmatrix}
    \]
    Le système devient $AX = B \iff (A|B)$. $(A|B)$ est dit $A$ augmenté de $B$ et est la matrice augmenté du système.
\end{definition}

\subsubsection{Forme échelonné d'un système}
\noindent
Une matrice est de la forme échelonné si \begin{enumerate}
    \item La ligne qui précède une ligne non-nulle est non-nulle
    \item Le premier coefficient non-nul d'une ligne non-nulle, appellé le pivot, est plus à
          gauche que le premier coefficient non-nul de la ligne suivante.
\end{enumerate}
On peut simplifier le système si on obtient la forme échelonné de la matrice augmenté. On obtient la forme échelonné
en appliquer des opérations élémentaires sur les lignes de $A$ et $B$. Ces opérations sont \begin{enumerate}
    \item Échanger deux lignes, $L_j \leftrightarrow L_k$
    \item Additionner le multiple d'une ligne à une autre $L_j \mapsto L_j + \alpha L_k$
    \item Multiplier une ligne par un scalaire non-nul, $L_j \mapsto \alpha L_j, \ \alpha \neq 0$
\end{enumerate}

\subsubsection{Rang d'une matrice}
$\text{rg}(A)$ est le rang de $A$ et est le nombre de ligne non-nulle de la forme échelonné
de $A$. On a alors trois cas pour un système linéaire $AX = B$, soit \begin{enumerate}
    \item Si $\text{rg}(A) < rg(A|B)$, alors le système n'a pas de solution, il est incompatible
    \item Si $\text{rg}(A) = rg(A|B) = n$, alors le système a une unique solution, il est compatible
    \item Si $\text{rg}(A) = rg(A|B) < n$, alors le système a une infinité de solution, il est compatible
\end{enumerate}
Dans le dernier cas, le système a une infinité de solution puisque qu'il a des variables libres,
soit exactement $n - \text{rg}(A)$ variables libres.
\begin{remark}
    Si $A^\prime$ est la forme échelonné de $A$, alors $\det A = \alpha \det A^\prime, \ \alpha \in \C$.
    On peut donc étendre le théorème \ref{inversible_small} avec le rang
\end{remark}
\begin{theorem}
    $\det A \neq 0 \iff \text{rg}(A) = n \iff A $ est inversible $\iff AX=B $ a une unique solution
\end{theorem}
\begin{remark}
    L'unique solution dans ce cas est $X = A^{-1}B$, puisque $A$ est inversible.
\end{remark}

\subsubsection{Calculer l'inverse d'une matrice}
\noindent
Il est possible de calculer l'inverse de $A$ en échelonnant le système $(A|I)$. Le
système échelonné va donner $(A|I) \sim (I|B)$, avec $B = A^{-1}$.

\subsubsection{Système homogène}
\noindent
Un système homogène est un système de la forme $AX = \mathbb{O}$. Un tel système
est toujours compatible avec la solution trivial $X = \mathbb{O}$. De plus, si $A$ est inversible
le système homogène à seulement la solution trivial, sinon il a une infinité de
solution, puisque c'est impossible que $\text{rg}(A) < \text{rg}(A|\mathbb{O})$,
et donc que le système soit incompatible.

\subsubsection{Noyau d'un système}
\noindent
Soit $A \in M_{m \times n}$
\begin{definition}
    Le noyau de $A$, noté $\text{N}(A)$, est l'ensemble de tous les solutions du système homogène
    $AX = \mathbb{O}$, soit 
    \[ \text{N}(A) = \left\{ X \in M_{n \times 1} \ | \ AX = \mathbb{O}  \right\} \]
\end{definition}